# -*- coding: utf-8 -*-
"""CeurRAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G8tQJGXL-Dv7oszsnM8uR61OlL1GdKxt
"""

import os
import requests
from bs4 import BeautifulSoup

BASE_URL = "http://ceur-ws.org"
SAVE_DIR = "ceur_pdfs"
os.makedirs(SAVE_DIR, exist_ok=True)

# Load list of already-downloaded volumes (if any)
done_volumes = set()
if os.path.exists("downloaded_volumes.txt"):
    with open("downloaded_volumes.txt") as f:
        done_volumes = set(line.strip() for line in f)

# Crawl volumes in a given range (adjust as needed)
for vol in range(1, 4000):
    if vol in done_volumes:
        continue  # skip volumes already processed
    vol_url = f"{BASE_URL}/Vol-{vol}/"
    try:
        res = requests.get(vol_url)
    except requests.RequestException:
        continue
    if res.status_code != 200:
        continue  # volume does not exist
    soup = BeautifulSoup(res.text, 'html.parser')
    # Find all PDF links on the page
    pdf_links = []
    for a in soup.select("a"):
        href = a.get("href", "")
        if href.endswith(".pdf"):
            pdf_links.append(href if href.startswith("http") else BASE_URL + "/" + href.lstrip("/"))
    if not pdf_links:
        continue  # no papers found
    # Download each PDF if not already saved
    vol_dir = os.path.join(SAVE_DIR, f"Vol-{vol}")
    os.makedirs(vol_dir, exist_ok=True)
    for pdf_url in pdf_links:
        filename = os.path.basename(pdf_url)
        save_path = os.path.join(vol_dir, filename)
        if not os.path.exists(save_path):
            try:
                pdf_res = requests.get(pdf_url)
                if pdf_res.status_code == 200:
                    with open(save_path, "wb") as f:
                        f.write(pdf_res.content)
                    print(f"Downloaded {save_path}")
            except requests.RequestException:
                print(f"Failed to download {pdf_url}")
    # Mark this volume as done
    with open("downloaded_volumes.txt", "a") as f:
        f.write(f"{vol}\n")

# LangChain commynity modul
!pip install -U langchain langchain-community

# PDF
!pip install pypdf

# Embedding with Sentence-transformers
!pip install sentence-transformers

# ChromaDB
!pip install chromadb

#Connection between Ollama and LLaMA (MUST BE LOCAL ‚Äì Cannot be used directly on Colab)
#Ollama does not work on Colab; it only runs in a local Python environment.



#Required installations
!pip install beautifulsoup4 requests



from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

CHUNK_SIZE = 2000
CHUNK_OVERLAP = 200

documents = []

for root, _, files in os.walk(SAVE_DIR):
    for fname in files:
        if fname.lower().endswith(".pdf"):
            path = os.path.join(root, fname)
            try:
                loader = PyPDFLoader(path)
                pages = loader.load()
                splitter = RecursiveCharacterTextSplitter(
                    chunk_size=CHUNK_SIZE,
                    chunk_overlap=CHUNK_OVERLAP
                )
                chunks = splitter.split_documents(pages)
                for chunk in chunks:
                    chunk.metadata = {"source": path, **chunk.metadata}
                documents.extend(chunks)
            except Exception as e:
                print(f"‚ùå Skipped {path}: {e}")

from sentence_transformers import SentenceTransformer

# Load a sentence-transformer model
model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')

# Compute embeddings for all chunks
texts = [doc.page_content for doc in documents]
embeddings = model.encode(texts, show_progress_bar=True)

# Prepare metadata (only safe fields)
metadatas = []
for doc in documents:
    # Filtrele: sadece string/int/float/bool/None
    clean_meta = {}
    for key, value in doc.metadata.items():
        if isinstance(value, (str, int, float, bool)) or value is None:
            clean_meta[key] = value
        else:
            clean_meta[key] = str(value)  # convert to String if needed
    metadatas.append(clean_meta)

import chromadb

# Metadata cleaning process
clean_metadatas = []
for doc in documents:
    clean_meta = {}
    for key, value in doc.metadata.items():
        if isinstance(value, (str, int, float, bool)) or value is None:
            clean_meta[key] = value
        else:
            clean_meta[key] = str(value)  # Diƒüer tipleri string yap
    clean_metadatas.append(clean_meta)

# ChromaDB Insert
chroma_client = chromadb.PersistentClient(path="chromadb_store")
collection = chroma_client.get_or_create_collection(name="ceur_documents")

ids = [f"doc_{i}" for i in range(len(documents))]
texts = [doc.page_content for doc in documents]
embs = [emb.tolist() for emb in embeddings]

batch_size = 1000
for i in range(0, len(texts), batch_size):
    batch_ids = ids[i:i+batch_size]
    batch_texts = texts[i:i+batch_size]
    batch_embs = embs[i:i+batch_size]
    batch_metas = clean_metadatas[i:i+batch_size]

    collection.upsert(
        ids=batch_ids,
        embeddings=batch_embs,
        documents=batch_texts,
        metadatas=batch_metas
    )

print(f"‚úÖ Inserted {len(texts)} chunks into ChromaDB.")

!pip install transformers torch chromadb sentence-transformers

from transformers import pipeline
import chromadb
from sentence_transformers import SentenceTransformer

# ChromaDB
chroma_client = chromadb.PersistentClient(path="chromadb_store")
collection = chroma_client.get_collection(name="ceur_documents")

# Embedding Model
model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')

# Hugging Face QA Model
qa_model = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")

def rag_query(question, top_k=5):
    query_emb = model.encode(question).tolist()
    results = collection.query(query_embeddings=[query_emb], n_results=top_k)
    context = " ".join(results['documents'][0])

    result = qa_model(question=question, context=context)
    return result['answer']

# Sample Usage
print(rag_query("What is the purpose of CEUR-WS?"))

# Install required packages
!pip install transformers torch chromadb sentence-transformers

# Import dependencies
from transformers import pipeline
import chromadb
from sentence_transformers import SentenceTransformer

# Load ChromaDB vector database
chroma_client = chromadb.PersistentClient(path="chromadb_store")
collection = chroma_client.get_collection(name="ceur_documents")

# Load embedding model
embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')

# Load Hugging Face QA model (distilled model for efficiency)
qa_pipeline = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")

# Define RAG query function
def rag_query(question, top_k=5):
    # Encode question into embedding
    query_embedding = embedding_model.encode(question).tolist()

    # Query ChromaDB for relevant contexts
    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)

    # Concatenate top retrieved documents as context
    context = " ".join(results['documents'][0])

    # Perform question answering using retrieved context
    result = qa_pipeline(question=question, context=context)

    return result['answer']

# Run an example query
answer = rag_query("What is AI?")
print("üîç Purpose of AI:", answer)